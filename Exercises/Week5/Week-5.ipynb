{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qU4Y9G1pZFW"
   },
   "source": [
    "# ðŸ“š  Exercise Session - Week 5\n",
    "\n",
    "Welcome to Week 5 exercise session of CS552-Modern NLP!\n",
    "\n",
    "We will continue playing with `DistilBert` this week, and learn about the dataset biases and prompting.\n",
    "\n",
    "[Part 1: Biases](#bias0)\n",
    "- [1.1 Hypothesis only NLI](#bias1)\n",
    "\n",
    "[Part 2: Prompting](#prompt0)\n",
    "- [2.1 Zero-shot Prompting](#promp1)\n",
    "- [2.2 Few-shot Prompting](#promt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be5UoVVfbUJl"
   },
   "source": [
    "### 0. Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741599713980,
     "user": {
      "displayName": "Simin Fan",
      "userId": "15154949338491351516"
     },
     "user_tz": -60
    },
    "id": "zQzHMwP6bV4p",
    "outputId": "9cce71a3-16bc-4d37-9f6d-c11d5f048c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device on: [cpu]\n"
     ]
    }
   ],
   "source": [
    "## Set up the device\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built(): # enable the usage of Apple silicon\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device on: [{DEVICE}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PgcwMKpZY4"
   },
   "source": [
    "<a name=\"bias0\"></a>\n",
    "## 1. Biases\n",
    "\n",
    "Recall our knowledge about the NLI tasks, the model would be given a pair of sentence: `(premise, hypothesis)`, and needs to judge the relationship between them. Specifically, given the *premise*, if the *hypothesis* is **true (entailment)**, **false (condradiction)**, or **neither (neutral)**. Idealy, The label of the hypothesis should be entirely based upon the given premise. However, *if the model is able to correctly guess the label without seeing the premise, it is likely detecting biased statisitcal patterns that are undesirable*, such as tendency to use certain words among different classes (ex: using negation words such as 'not' for the contradiction label).\n",
    "\n",
    "Inspired by the paper [Hypothesis Only Baselines in Natural Language Inference](https://aclanthology.org/S18-2023.pdf), the first part of this lab will investigate a classifier's internal bias when performing the NLI task by testing its hypothesis-only performance.\n",
    "\n",
    "**`Note`** In this dataset the labels are as follows: `0-Entailment`, `1- Neutral`, and `2- Contradict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Yb7ZmtDHyKAb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonpickle\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import RobertaForMaskedLM,RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3I7D8ul5ikM"
   },
   "source": [
    "<a name=\"bias1\"></a>\n",
    "## 1.1 Train: Hypothesis only NLI\n",
    "\n",
    "Let's firstly train a `distilbert` model on the SNLI dataset, but only access the hypothesis. We reuse the functions from the Exercise4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mgAGjzpYn-Ws"
   },
   "outputs": [],
   "source": [
    "def load_pretrained(model_name, num_labels=2):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "  # Map the model onto targeted (predefined) DEVICE\n",
    "  model = model.to(DEVICE)\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LnsR5KXGntJd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training samples:  549367\n",
      "#Test samples:  9824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"snli\", split='train')\n",
    "test_dataset = load_dataset(\"snli\", split='test')\n",
    "train_dataset = train_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
    "test_dataset = test_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
    "print('#Training samples: ', len(train_dataset))\n",
    "print('#Test samples: ', len(test_dataset))\n",
    "\n",
    "tokenizer, model = load_pretrained('distilbert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sbNnn1CMnzhC"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X7pBz6KopBNN"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_nli(model, tokenizer, test_loader):\n",
    "  all_labels = None\n",
    "  all_preds = None\n",
    "\n",
    "  for b in tqdm(test_loader):\n",
    "    premise = b['premise']\n",
    "    hypothesis = b['hypothesis']\n",
    "    label = b['label']\n",
    "\n",
    "    # step1: tokenize the text\n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    label = label.to(DEVICE)\n",
    "\n",
    "    # step2: run the model to make the prediction\n",
    "    with torch.no_grad():\n",
    "        pred = model(**inputs).logits.argmax(dim=-1)\n",
    "\n",
    "    if all_labels is None:\n",
    "      all_labels = label.cpu()\n",
    "      all_preds = pred.cpu()\n",
    "    else:\n",
    "      all_labels = torch.concat([all_labels, label.cpu()])\n",
    "      all_preds = torch.concat([all_preds, pred.cpu()])\n",
    "\n",
    "  assert len(all_preds)==len(all_labels), 'Test Failed. Check your code!'\n",
    "  # step3: compute eval metrices\n",
    "  # compute f1 score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
    "  f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "  # compute accuracy score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
    "  acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "  # compute the accuracy on Entailment(label==0) samples\n",
    "  entailment_acc = accuracy_score(all_labels[all_labels==0], all_preds[all_labels==0])\n",
    "\n",
    "  # compute the accuracy on Neutral(label==1) samples\n",
    "  neutral_acc = accuracy_score(all_labels[all_labels==1], all_preds[all_labels==1])\n",
    "\n",
    "  # compute the accuracy on Contradict(label==1) samples\n",
    "  contradict_acc = accuracy_score(all_labels[all_labels==2], all_preds[all_labels==2])\n",
    "\n",
    "  print('Accuracy: ', acc*100, '%')\n",
    "  print(' -- Entailment Accuracy: ', entailment_acc*100, '%')\n",
    "  print(' -- Neutral Accuracy: ', neutral_acc*100, '%')\n",
    "  print(' -- Contradict Accuracy: ', contradict_acc*100, '%')\n",
    "  print('F1 score: ', f1)\n",
    "\n",
    "  return all_preds, all_labels, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xbMBYWF0IHPQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [02:32<00:00,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  33.876221498371336 %\n",
      " -- Entailment Accuracy:  81.59144893111639 %\n",
      " -- Neutral Accuracy:  17.707362534948743 %\n",
      " -- Contradict Accuracy:  0.3089280197713933 %\n",
      "F1 score:  0.23918460087939894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ETS: <1min on colab T4 gpu\n",
    "\n",
    "# Since the parameters of the classification head are not trained now, the results of this initial evaluation are basically random :)\n",
    "all_preds, all_labels, acc, f1 = evaluate_model_nli(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ezY5R5mCMs"
   },
   "source": [
    "`TODO-1`: Implement the `tokenize_function` to tokenize only the `hypothesis` in each input `examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOygmXHFIOCF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 549367/549367 [00:33<00:00, 16373.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9824/9824 [00:00<00:00, 17387.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define a function to tokenize the text\n",
    "def tokenize_function(examples, hyp_only=True, max_length=512, device=DEVICE):\n",
    "  '''\n",
    "  INPUT:\n",
    "    examples: input samples in the dataset\n",
    "    hyp_only: if True, only tokenize the \"hypothesis\"; tokenize both \"premise\" and \"hypothesis\" if False\n",
    "    max_length: maximal number of tokens\n",
    "    device: cuda or cpu or mps (default as the pre-defined DEVICE)\n",
    "  OUTPUT:\n",
    "    tokenized: tokenized sample, truncation=True, padding=True\n",
    "  '''\n",
    "  if hyp_only:\n",
    "    # TODO\n",
    "        tokenized = tokenizer(examples[\"hypothesis\"], max_length=max_length, padding=True, truncation = True)\n",
    "  else:\n",
    "    # TODO \n",
    "    tokenized = tokenizer(examples[\"premise\"], examples[\"hypothesis\"], max_length=max_length, padding=True, truncation = True)\n",
    "  return tokenized\n",
    "\n",
    "# Tokenize the train and test data\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched = True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched = True)\n",
    "\n",
    "# Define a data collator to handle padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vQcmZaRsJj4P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12692\\3853724977.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Import the trainer and training arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define the output directory and other training arguments\n",
    "output_dir_name = \"snli-hyp-distilbert\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir = output_dir_name,\n",
    "   learning_rate = 2e-5,\n",
    "   per_device_train_batch_size = 16,\n",
    "   per_device_eval_batch_size = 16,\n",
    "   num_train_epochs = 1,\n",
    "   max_steps = 5000,\n",
    "   weight_decay = 0.01,\n",
    "   save_strategy = \"steps\",\n",
    "   save_steps = 500,\n",
    "   push_to_hub = False,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "   model = model,\n",
    "   args = training_args,\n",
    "   train_dataset = tokenized_train,\n",
    "   tokenizer = tokenizer,\n",
    "   data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C0qUHJeC0h6q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "API key must be 40 characters long, yours was 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\trainer.py:2437\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2435\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m   2436\u001b[0m grad_norm: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_on_start:\n\u001b[0;32m   2440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\trainer_callback.py:469\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m    468\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\trainer_callback.py:519\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 519\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, event)(\n\u001b[0;32m    520\u001b[0m             args,\n\u001b[0;32m    521\u001b[0m             state,\n\u001b[0;32m    522\u001b[0m             control,\n\u001b[0;32m    523\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    524\u001b[0m             processing_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class,\n\u001b[0;32m    525\u001b[0m             optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer,\n\u001b[0;32m    526\u001b[0m             lr_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler,\n\u001b[0;32m    527\u001b[0m             train_dataloader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader,\n\u001b[0;32m    528\u001b[0m             eval_dataloader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataloader,\n\u001b[0;32m    529\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    530\u001b[0m         )\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\integrations\\integration_utils.py:916\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[1;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup(args, state, model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\integrations\\integration_utils.py:843\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[1;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[0;32m    837\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    838\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    839\u001b[0m             repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    840\u001b[0m         )\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m    844\u001b[0m         project\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_PROJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    845\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_args,\n\u001b[0;32m    846\u001b[0m     )\n\u001b[0;32m    847\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1485\u001b[0m, in \u001b[0;36minit\u001b[1;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[0;32m   1481\u001b[0m     wl\u001b[38;5;241m.\u001b[39m_get_logger()\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[1;32m-> 1485\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\analytics\\sentry.py:156\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1433\u001b[0m, in \u001b[0;36minit\u001b[1;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[0;32m   1429\u001b[0m wl \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m   1431\u001b[0m wi \u001b[38;5;241m=\u001b[39m _WandbInit(wl, init_telemetry)\n\u001b[1;32m-> 1433\u001b[0m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1434\u001b[0m run_settings \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39mmake_run_settings(init_settings)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_settings\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:175\u001b[0m, in \u001b[0;36m_WandbInit.maybe_login\u001b[1;34m(self, init_settings)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_settings\u001b[38;5;241m.\u001b[39m_noop \u001b[38;5;129;01mor\u001b[39;00m run_settings\u001b[38;5;241m.\u001b[39m_offline:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[43mwandb_login\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manonymous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_disable_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_silent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\sdk\\wandb_login.py:308\u001b[0m, in \u001b[0;36m_login\u001b[1;34m(anonymous, key, relogin, host, force, timeout, verify, _silent, _disable_warning)\u001b[0m\n\u001b[0;32m    305\u001b[0m     wlogin\u001b[38;5;241m.\u001b[39m_verify_login(key)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key_is_pre_configured:\n\u001b[1;32m--> 308\u001b[0m     \u001b[43mwlogin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m     wlogin\u001b[38;5;241m.\u001b[39mupdate_session(key, status\u001b[38;5;241m=\u001b[39mkey_status)\n\u001b[0;32m    310\u001b[0m     wlogin\u001b[38;5;241m.\u001b[39m_update_global_anonymous_setting()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\sdk\\wandb_login.py:175\u001b[0m, in \u001b[0;36m_WandbLogin.configure_api_key\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    168\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre specifying your api key in code, ensure this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode is not shared publicly.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_API_KEY environment variable, or running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`wandb login` from the command line.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m     )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[1;32m--> 175\u001b[0m     \u001b[43mapikey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\wandb\\sdk\\lib\\apikey.py:241\u001b[0m, in \u001b[0;36mwrite_key\u001b[1;34m(settings, key, api)\u001b[0m\n\u001b[0;32m    238\u001b[0m _, suffix \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(suffix) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m40\u001b[39m:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key must be 40 characters long, yours was \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(key))\n\u001b[0;32m    243\u001b[0m     )\n\u001b[0;32m    245\u001b[0m write_netrc(settings\u001b[38;5;241m.\u001b[39mbase_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n",
      "\u001b[1;31mValueError\u001b[0m: API key must be 40 characters long, yours was 2"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QuM-1JVa9mkm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [02:31<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  33.27565146579804 %\n",
      " -- Entailment Accuracy:  63.42042755344418 %\n",
      " -- Neutral Accuracy:  29.574401988195092 %\n",
      " -- Contradict Accuracy:  5.591597157862218 %\n",
      "F1 score:  0.28203308929406345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ETS: <1min on colab T4 gpu\n",
    "all_preds, all_labels, acc, f1 = evaluate_model_nli(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihq84X2nSe2Q"
   },
   "source": [
    "As you see, the model is able to correctly guess the labels of almost **60-70%** of the NLI hypotheses without seeing what the premise is.\n",
    "\n",
    "Question: What do you think are ways that biases can be mitigated? Think about both the data collection process and model training for places where one can intervene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6NTm7cJ8dqb"
   },
   "source": [
    "<a name=\"prompt0\"></a>\n",
    "## 2. Prompting\n",
    "\n",
    "The following sections will be based on the papers [Exploiting Cloze Questions for Few Shot Text Classification and Natural\n",
    "Language Inference](https://arxiv.org/pdf/2001.07676.pdf) and [How Many Data Points is a Prompt Worth?](https://arxiv.org/pdf/2103.08493.pdf).\n",
    "\n",
    "The first paper introduced Pattern Exploiting Training (PET), in which a NLP task is reformulated to a cloze style task for few shot learning. We will go into this a little more during the few-shot section of this lab.\n",
    "\n",
    "The basic idea is to only tune a linear classification head (mostly be a MLP layer, attached on the top of original pretrained model) instead of training the entire model to perform the classification task. Unlike language modeling, which predicts the next-token from the whole vocaulary, we are predicting a word from a list of **verbalizers**, where each verbalizer corresponds to one label.\n",
    "\n",
    "### NLI and Sentiment classification\n",
    "We will be looking at classification tasks (NLI and sentiment) where we only need a single word verbalizers. However this paradigm can be extended to more complex tasks, with multi-token verbalizers.\n",
    "\n",
    "First lets try **zero-shots prompting**. We will use `Roberta-large` for this section and investigate an easier `sentiment-analysis` task on IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EtDDLw-ryYEu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset('imdb', split='test')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deV4pRt2noeF"
   },
   "source": [
    "`TODO-2`: Complete the `lm_guess_sent` function to get the probability of each verbalizer in the `<mask>`, then make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CildcznAK3h6"
   },
   "outputs": [],
   "source": [
    "def get_targets(verbalizer = 1):  #retreives the token ids for the verbalizers\n",
    "  targets = verbalize(verbalizer).keys()\n",
    "  target_ids = []\n",
    "  for target in targets:\n",
    "    id= tokenizer.get_vocab().get(\"\\u0120\"+ target, None) #how roberta ecodes wods\n",
    "    target_ids.append(id)\n",
    "  return target_ids\n",
    "\n",
    "def lm_guess_sent(model, text, template_num = 1, verb_num = 1, context_samples = None, context_labels = None, device=DEVICE):\n",
    "  model = model.to(device)\n",
    "\n",
    "  verbalizer = verbalize(verb_num) # choose a pair of verbalizers\n",
    "  target_ids = get_targets(verb_num) # get ids of verbalizers\n",
    "  text_template = template(text, template_num, context_samples=context_samples, context_labels=context_labels) # get a template with text\n",
    "\n",
    "  # TODO: encode texts with the template (text_template), return tensor\n",
    "  encoded_input = tokenizer(text_template, return_tensors='pt', padding='longest', truncation=True).to(device)\n",
    "\n",
    "  masked_index = torch.nonzero(encoded_input[\"input_ids\"][0] == tokenizer.mask_token_id, as_tuple=False).squeeze(-1).to(device) #getting index of mask token\n",
    "  model_outputs = model(**encoded_input)\n",
    "  outputs = model_outputs[\"logits\"]\n",
    "\n",
    "  # TODO: get the logits for masked tokens\n",
    "  logits = outputs[0, masked_index, :] # get the logits for the masked\n",
    "\n",
    "  probs = logits.softmax(dim=-1) # probability of tokens\n",
    "\n",
    "  # TODO: get the probability of the two verbalizer tokens\n",
    "  probs = probs[..., target_ids]\n",
    "\n",
    "  # TODO: get prediction as the index with higher probability\n",
    "  _, predictions = probs.topk(1)\n",
    "  input_ids = encoded_input[\"input_ids\"][0]\n",
    "  tokens = input_ids.detach().cpu().numpy().copy()\n",
    "  p = target_ids[predictions]\n",
    "\n",
    "  prediction = verbalizer[tokenizer.decode([p]).strip()] #get corresponging label from verbalizer\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO_e65cMAslb"
   },
   "source": [
    "<a name=\"prompt1\"></a>\n",
    "### 2.1 Zero-shot Prompting\n",
    "\n",
    "\n",
    "We will be using the IMDB dataset again to test prompting in the zero shot setting.\n",
    "\n",
    "We need two things to do the prompting\n",
    "\n",
    "- a **Verbalizer** that matches a word to each label\n",
    "- a **Template** to add the review, with one masked token that will predict one of the verbalizers\n",
    "\n",
    "Success of this method varies by template and verbalizer, so it is nice to test a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fekboRQ1d6xL"
   },
   "outputs": [],
   "source": [
    "def verbalize(num = 1):\n",
    "  if num == 1:\n",
    "    return {\"great\":1, \"horrible\":0}\n",
    "  if num == 2:\n",
    "    return {\"great\":1, \"terrible\":0}\n",
    "\n",
    "\n",
    "def template(text, num = 1, context_samples=None, context_labels=None):\n",
    "    if num == 1:\n",
    "      return \"It was <mask>.\" + text\n",
    "    if num == 2:\n",
    "      return \"So <mask>!\" + text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ewv9hKEejyZP"
   },
   "source": [
    "Alright, lets see how the pre-trained roberta does on the prompted sentiment analysis.\n",
    "\n",
    "#### Verbalizer #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WQUxhDj6Ojqo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1076) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1076].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data_subset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_dataset[random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_dataset)), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)])\n\u001b[1;32m----> 3\u001b[0m guess \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data_subset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_guess_sent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data_subset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_dataset[random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_dataset)), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)])\n\u001b[1;32m----> 3\u001b[0m guess \u001b[38;5;241m=\u001b[39m test_data_subset\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mlm_guess_sent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mlm_guess_sent\u001b[1;34m(model, text, template_num, verb_num, context_samples, context_labels, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m encoded_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m masked_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m#getting index of mask token\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\n\u001b[0;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# TODO: get the logits for masked tokens\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1206\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1219\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1220\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:907\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    906\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[1;32m--> 907\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1076) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1076].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "test_data_subset = pd.DataFrame(test_dataset[random.choices(range(len(test_dataset)), k=500)])\n",
    "\n",
    "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 1), axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDyc3IpDkIGR",
    "outputId": "1998b806-48a2-46b7-efcc-5cdccd97758d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.878\n",
      "Positive Accuracy : 0.95\n",
      "Negative Accuracy : 0.8115384615384615\n",
      "F1 : 0.878\n"
     ]
    }
   ],
   "source": [
    "test_data_subset['guess'] = guess\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['guess']))\n",
    "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['guess']))\n",
    "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['guess']))\n",
    "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['guess'], average = 'micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2PyYK7LzDRE"
   },
   "source": [
    "It seems the first verbalizer works better for the Positive reviews. How can we improve the performance without retrain or finetune the model?\n",
    "\n",
    "#### Verbalizer #2\n",
    "Lets try different verbalizers (selection 2).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf8yGe65y8uU"
   },
   "outputs": [],
   "source": [
    "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 2), axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWubfN7Cy87R",
    "outputId": "289304c9-152f-4b55-83a9-b7adb2d97752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.898\n",
      "Positive Accuracy : 0.9125\n",
      "Negative Accuracy : 0.8846153846153846\n",
      "F1 : 0.898\n"
     ]
    }
   ],
   "source": [
    "test_data_subset['guess2'] = guess\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['guess2']))\n",
    "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['guess2']))\n",
    "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['guess2']))\n",
    "\n",
    "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['guess2'], average = 'micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgj5mdSZvknh"
   },
   "source": [
    "**Thinking**: The second verbalizer seems work well with ~90% accuracy in both classes! Why do you think this formulation of the task works in the zero-shot setting? Can you think of any ways to *pick the most effective verbalizers* in a more systematic way?\n",
    "\n",
    "You can feel free to try your own templates/verbalizers to see how your design choices affect performance, and which ones could improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbsGhu95ux4s"
   },
   "source": [
    "<a name=\"prompt2\"></a>\n",
    "### 2.2 Few-shot Prompting\n",
    "\n",
    "Now given that we have an access to a very small labeled dataset (e.g. 5 samples), how can we make a great use of these information?\n",
    "\n",
    "If we finetune the model on these 5 samples, the model is very likely to overfit to some biased shortcuts. **Recall the prompting trick, do we have some ways to re-design the template to combine the labeled samples?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY3UnDeMI1D9"
   },
   "outputs": [],
   "source": [
    "train_data = load_dataset('imdb', split='train')\n",
    "train_data = train_data.shuffle(seed=42)\n",
    "fewshot_samples = train_data.select(range(10))\n",
    "\n",
    "context_samples = fewshot_samples['text']\n",
    "context_labels = fewshot_samples['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9Nh96XoW-t8"
   },
   "outputs": [],
   "source": [
    "def verbalize(num = 1):\n",
    "  if num == 1:\n",
    "    return {\"great\":1, \"horrible\":0}\n",
    "  if num == 2:\n",
    "    return {\"great\":1, \"terrible\":0}\n",
    "\n",
    "\n",
    "def template(text, num = 1, context_samples = None, context_labels = None):\n",
    "    if num == 1:\n",
    "      temp = \"It was <mask>.\" + text\n",
    "      pos_prefix = \"It was great.\"\n",
    "      neg_prefix = \"It was horrible.\"\n",
    "    elif num == 2:\n",
    "      temp = \"So <mask>!\" + text\n",
    "      pos_prefix = \"It was great.\"\n",
    "      neg_prefix = \"It was terrible.\"\n",
    "    else:\n",
    "      raise NotImplemented\n",
    "\n",
    "    # Build 'Context' with few-shot labeled samples\n",
    "    if context_samples is not None:\n",
    "      assert context_labels is not None, 'Please provide labels to the few-shot samples!'\n",
    "      context = ''\n",
    "      for c,y in zip(context_samples, context_labels):\n",
    "        if y==0:\n",
    "          context += (neg_prefix+' '.join(c.split(' ')[:25])+'//')\n",
    "        elif y==1:\n",
    "          context += (pos_prefix+' '.join(c.split(' ')[:25])+'//')\n",
    "      return context+temp\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "VpqnmIEzI8a7",
    "outputId": "597ac396-98e7-4197-979b-8ff4c06272f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'It was great.There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier//It was great.This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of//It was horrible.George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to//It was great.In the process of trying to establish the audiences\\' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never//It was horrible.Yeh, I know -- you\\'re quivering with excitement. Well, *The Secret Lives of Dentists* will not upset your expectations: it\\'s solidly made but essentially unimaginative,//It was great.While this movie\\'s style isn\\'t as understated and realistic as a sound version probably would have been, this is still a very good film. In//It was great.I give this movie 7 out of 10 because the villains were interesting in their roles and the unknown batwoman creates an interesting \"guess who\"//It was horrible.really awful... lead actor did OK... the film, plot etc was completely crap and inaccurate it may as well have been a sequel to well...//It was horrible.Good grief I can\\'t even begin to describe how poor this film is. Don\\'t get me wrong, I wasn\\'t expecting much to begin with. Let\\'s//It was great.Home Room deals with a Columbine-like high-school shooting but rather than hashing over the occurrence itself the film portrays the aftermath and what happened to//It was <mask>.OK, this simply is the worst movie ever made. Period. Horrible acting, sets and music. Ok, everything sucks in this movie. I almost forgot! The special effects are \"great\" also. So if you like bad movies, watch this, it can surely make you laugh!!'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how the template would look like\n",
    "template(test_data_subset['text'][0], num = 1, context_samples = context_samples, context_labels = context_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc8rGlX4wbdX"
   },
   "source": [
    "Then we can test how the model performs with **few-shot prompting**.\n",
    "\n",
    "#### Verbalizer number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1DzL_36KkDG"
   },
   "outputs": [],
   "source": [
    "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 1, context_samples=context_samples, context_labels=context_labels), axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MtrsFIehPko",
    "outputId": "2b2e15bc-722b-43c4-eeca-43a033d91422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.556\n",
      "Positive Accuracy : 1.0\n",
      "Negative Accuracy : 0.059322033898305086\n",
      "F1 : 0.556\n"
     ]
    }
   ],
   "source": [
    "test_data_subset['10shots-guess'] = guess\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['10shots-guess']))\n",
    "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['10shots-guess']))\n",
    "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['10shots-guess']))\n",
    "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['10shots-guess'], average = 'micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGeQANEOY7Bu"
   },
   "source": [
    "#### Verbalizer number 2\n",
    "\n",
    "Now we will use different verbalizers to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULH7ghzWjZ-k"
   },
   "outputs": [],
   "source": [
    "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 2, context_samples=context_samples, context_labels=context_labels), axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaTn6mhAjaYd",
    "outputId": "7c245345-5a25-40d7-9dcd-019879e29bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.528\n",
      "Positive Accuracy : 1.0\n",
      "Negative Accuracy : 0.0\n",
      "F1 : 0.528\n"
     ]
    }
   ],
   "source": [
    "test_data_subset['10shots-guess2'] = guess\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['10shots-guess2']))\n",
    "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['10shots-guess2']))\n",
    "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['10shots-guess2']))\n",
    "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['10shots-guess2'], average = 'micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN5VRZ2hlCO-"
   },
   "source": [
    "**Thinking**: What do you think of the performance? Why do you think it could happen? What can we do to improve?\n",
    "\n",
    "\n",
    "Feel free to process or change the prompts/contexts as you like, then you can see how your design choices could influence the few-shot prompting performance :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
