{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3YaGFE03v1"
   },
   "source": [
    "# ðŸ“š Exercise Session - Week 3: Attention + Transformers\n",
    "**Main Topics**: Attention & Transformers for Sequence-to-Sequence Modeling\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This weekâ€™s session dives into Transformers for sequence-to-sequence (Seq2Seq) tasks, with a special focus on machine translation and attention visualization. By the end, you will have built and trained a Transformer model capable of translating text from one language to anotherâ€”and youâ€™ll see how attention helps it learn alignments across tokens.\n",
    "\n",
    "1. [**TASK A:** Transformer Implementation](#Task-A:-Transformer-Implementation)\n",
    "- Build a BPE tokenizer\n",
    "- Implement a Transformer encoder-decoder model using PyTorch\n",
    "\n",
    "2. [**TASK B:** Train a Machine Translation Model](#Task-B:-Train-a-Machine-Translation-Model)\n",
    "- Use Transformer from Task A to train a machine translation model\n",
    "- Visualize the cross-attention weights\n",
    "\n",
    "3. **Optional Extensions** \n",
    "- Learning Rate Scheduler: Try adding a scheduler (like Warmup or ReduceLROnPlateau) to potentially improve convergence.\n",
    "- Periodic Validation: Every `m` batches or at epochâ€™s end, evaluate on a validation set to track loss or metrics like BLEU.\n",
    "- Use PyTorch `DataLoader`\n",
    "\n",
    "**Tips & Hints**\n",
    "- **Overfitting on a Single Example**: If your model cannot easily learn one sample to near-perfect accuracy, it often indicates an implementation bug or mismatch in shapes/masks.\n",
    "- **Masking**: Pay careful attention to causal masks in the decoder, ensuring the model does not see future tokens.\n",
    "- **Debugging**: Print shapes and partial outputs, or watch the attention scores to confirm they behave as expected.\n",
    "\n",
    "\n",
    "> **By the end of the session you will be able to:**\n",
    "> - âœ…  Implement an encoder-decoder Transformer model using PyTorch\n",
    "> - âœ…  Train your model on a machine translation corpus\n",
    "> - âœ…  Understand attention mechanism within the Transformer architecture\n",
    "> - âœ…  Be more interested in NLP ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yDoUnLJKoIK"
   },
   "source": [
    "## Task A: Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Reminder**\n",
    "We will be implementing the following encoder-decoder transformer architecture following the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper!\n",
    "\n",
    "![Transformer Architecture](transformer_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kH-A3gdHF7o"
   },
   "source": [
    "In this part, you will implement an encoder-decoder Transformer model using [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T14:42:37.478843Z",
     "iopub.status.busy": "2024-02-22T14:42:37.478185Z",
     "iopub.status.idle": "2024-02-22T14:42:50.715331Z",
     "shell.execute_reply": "2024-02-22T14:42:50.714317Z",
     "shell.execute_reply.started": "2024-02-22T14:42:37.478777Z"
    },
    "id": "vFN2GC6PX1_P",
    "outputId": "aa984dba-d338-4f62-eaba-1c8006dc623e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # These layers transform the input embeddings to queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final projection after concatenating heads\n",
    "        self.out_proj   = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query, key, value: [batch_size, seq_len, embed_dim]\n",
    "        mask: [batch_size, 1, seq_len, seq_len] or None (optional)\n",
    "\n",
    "        returns:\n",
    "          - output: [batch_size, seq_len, embed_dim]\n",
    "          - attn_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Linear projections\n",
    "        # TODO: project query, key, and value via the layers: self.query_proj, self.key_proj, self.value_proj\n",
    "        # shape after projection: [batch_size, seq_len, embed_dim]\n",
    "        Q = self.query_proj(query)\n",
    "        K = self.key_proj(key)\n",
    "        V = self.value_proj(value)\n",
    "\n",
    "        # 2) Split into multiple heads\n",
    "        # We want shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        # TODO: reshape the Q, K, V so that we chunk embed_dim into (num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3) Scaled dot-product attention\n",
    "        #    attention_scores = Q x K^T / sqrt(head_dim)\n",
    "        #    then apply optional mask (if not None)\n",
    "        #    then softmax, then dropout, then multiply by V\n",
    "\n",
    "        # TODO: compute attention_scores\n",
    "        # attention_scores shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # TODO: apply mask if given\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # TODO: normalize attention_scores\n",
    "        # Softmax along the last dimension (seq_len of K)\n",
    "        attn_weights = F.softmax(attention_scores, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        \n",
    "        # TODO: compute attention_output\n",
    "        # multiply by V \n",
    "        # shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "        attention_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # 4) Concat heads\n",
    "        # We want shape: [batch_size, seq_len, embed_dim]\n",
    "        # (i.e. combine num_heads and head_dim back into embed_dim)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, -1, self.embed_dim)\n",
    "        \n",
    "        # 5) Final linear projection\n",
    "        output = self.out_proj(attention_output)\n",
    "        \n",
    "        return output, attn_weights  # (attn_weights = normalized_weights for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.linear1   = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2   = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.norm1     = nn.LayerNorm(embed_dim)\n",
    "        self.norm2     = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1  = nn.Dropout(dropout)\n",
    "        self.dropout2  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 1) Multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask)\n",
    "        x = x + attn_output  # residual\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 2) Feed-forward\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ff_output  # residual\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Multi-Head Attention Layers\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)   # Students already have MHA\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "\n",
    "        # Feed-Forward Layers\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "        # LayerNorms\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, tgt_seq_len, embed_dim] -- decoder input embeddings\n",
    "            enc_output: [batch_size, src_seq_len, embed_dim] -- encoder output\n",
    "            tgt_mask: mask for target self-attention (e.g., causal + padding)\n",
    "            cross_attn_mask: mask for encoder-decoder attention (padding mask for source)\n",
    "        Returns:\n",
    "            x: [batch_size, tgt_seq_len, embed_dim] -- updated decoder features\n",
    "            attn_weights: attention weights from cross-attention (for visualization, etc.)\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # 1) TODO: Implement cross-attention with encoder output\n",
    "        #    - Query = x (decoder), Key/Value = enc_output\n",
    "        #    - residual connection + self.norm2\n",
    "        cross_attn_output, attn_weights = self.cross_attn(x, enc_output, enc_output, mask=cross_attn_mask)\n",
    "        x = x + cross_attn_output\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # 2) TODO: Implement feed-forward sub-layer\n",
    "        #    - pass x through self.linear1, then an activation (e.g., F.relu)\n",
    "        #    - then self.linear2\n",
    "        #    - Add residual + self.norm3\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + ff_output\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        # TODO: return final x and the cross-attention weights\n",
    "        return x, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        src: [batch_size, src_seq_len]\n",
    "        src_mask: (optional)\n",
    "        \"\"\"\n",
    "        x = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        # TODO: Add positional encoding to the input embeddings\n",
    "        x = x + self.pos_encoding(torch.arange(x.size(1)).to(src.device)).unsqueeze(0)\n",
    "\n",
    "        # TODO: Implement the forward pass through the encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src)\n",
    "        \n",
    "        return x  # shape: [batch_size, src_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=1024, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, enc_output, tgt_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: [batch_size, tgt_seq_len]\n",
    "        enc_output: [batch_size, src_seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.embedding(tgt) * math.sqrt(self.embed_dim)\n",
    "        # TODO: Add positional encoding to the input embeddings\n",
    "        x = x + self.pos_encoding(torch.arange(tgt.size(1)).to(tgt.device)).unsqueeze(0)\n",
    "\n",
    "        attn_weights = None\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, enc_output, tgt_mask=tgt_mask, cross_attn_mask=cross_attn_mask)\n",
    "\n",
    "        # Final projection to vocabulary\n",
    "        logits = self.out_proj(x)  # [batch_size, tgt_seq_len, vocab_size]\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(seq_len, device=None):\n",
    "    \"\"\"\n",
    "    Returns a 2D causal mask of shape [seq_len, seq_len], \n",
    "    where True means 'allowed to attend' and False means 'disallowed'.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
    "\n",
    "def expand_causal_mask(causal_2d, batch_size, num_heads):\n",
    "    # causal_2d: shape [seq_len, seq_len]\n",
    "    # expand to [batch_size, num_heads, seq_len, seq_len]\n",
    "    causal_4d = causal_2d.unsqueeze(0).unsqueeze(0)  # => [1,1,seq_len,seq_len]\n",
    "    causal_4d = causal_4d.expand(batch_size, num_heads, causal_2d.size(0), causal_2d.size(1))\n",
    "    return causal_4d\n",
    "\n",
    "def expand_padding_mask(pad_mask_2d, num_heads):\n",
    "    # pad_mask_2d: shape [batch_size, seq_len], 1 = valid, 0 = pad\n",
    "    # step 1) Convert to bool if needed\n",
    "    pad_mask_bool = pad_mask_2d.bool()  # shape [batch_size, seq_len]\n",
    "    # step 2) unsqueeze => [batch_size, 1, 1, seq_len]\n",
    "    pad_mask_4d = pad_mask_bool.unsqueeze(1).unsqueeze(2)\n",
    "    # step 3) broadcast across the query dimension\n",
    "    batch_size, _, _, seq_len = pad_mask_4d.shape\n",
    "    pad_mask_4d = pad_mask_4d.expand(batch_size, num_heads, seq_len, seq_len)\n",
    "    return pad_mask_4d\n",
    "\n",
    "\n",
    "def build_decoder_mask(\n",
    "    pad_mask_2d: torch.Tensor, \n",
    "    num_heads: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine the target padding mask with the causal mask \n",
    "    to produce a final decoder mask of shape:\n",
    "    [batch_size, num_heads, seq_len, seq_len], \n",
    "    where True means 'allowed', False means 'masked out'.\n",
    "    \"\"\"\n",
    "    device = pad_mask_2d.device\n",
    "    batch_size, seq_len = pad_mask_2d.shape\n",
    "\n",
    "    # 1) Build the 2D causal mask\n",
    "    causal_2d = build_causal_mask(seq_len, device=device)\n",
    "\n",
    "    # 2) Expand to 4D\n",
    "    causal_4d = expand_causal_mask(causal_2d, batch_size, num_heads)\n",
    "\n",
    "    # 3) Expand the padding mask to 4D\n",
    "    pad_4d = expand_padding_mask(pad_mask_2d, num_heads)\n",
    "\n",
    "    # 4) Final mask = causal AND pad\n",
    "    final_mask = causal_4d & pad_4d  # shape [batch_size, num_heads, seq_len, seq_len]\n",
    "    return final_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=512, num_heads=8, hidden_dim=2048, \n",
    "                 num_layers=6, dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder = Encoder(src_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, cross_attn_mask=None):\n",
    "\n",
    "        if src_mask is not None:\n",
    "            src_mask = src_mask[:, None, None, :]\n",
    "            src_mask = src_mask.expand(-1, -1, src_mask.size(-1), -1)\n",
    "\n",
    "        tgt_mask = build_decoder_mask(tgt_mask, num_heads=self.num_heads)\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        logits, attn_weights = self.decoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Train a Machine Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to download the French-English translation dataset using [this link](https://drive.google.com/file/d/1cPKNjpU7PiqA33GzV0yICwDjZ_0ysKjO/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (2.4.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\miniconda3\\envs\\mnlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory wmt14_fr_en not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_from_disk, load_dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwmt14_fr_en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#dataset = load_dataset(\"wmt14\", \"fr-en\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\datasets\\load.py:2207\u001b[0m, in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[0;32m   2205\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[1;32m-> 2207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[0;32m   2209\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[0;32m   2210\u001b[0m ):\n\u001b[0;32m   2211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Directory wmt14_fr_en not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(\"wmt14_fr_en\")\n",
    "#dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (30/30 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40836715/40836715 [01:37<00:00, 419512.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00<00:00, 81097.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3003/3003 [00:00<00:00, 168092.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/path/to/wmt14_fr_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"/path/to/wmt14_fr_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a BPE Tokenizer\n",
    "\n",
    "> You will learn about BPE tokenization later in the semester. Here we provide you with the code necessary to train your own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Yields batches of text from the Hugging Face dataset.\n",
    "    Args:\n",
    "        dataset: a huggingface dataset split (e.g. train_dataset)\n",
    "        batch_size: how many samples per batch\n",
    "        text_column: name of the column containing the text\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        # Extract a batch of examples\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # 'batch' is now a list of strings (if text_column is indeed text).\n",
    "        yield batch\n",
    "\n",
    "\n",
    "fr_training_set = [dataset[\"train\"][i]['translation']['fr'] for i in range(100_000)]\n",
    "en_training_set = [dataset[\"train\"][i]['translation']['en'] for i in range(100_000)]\n",
    "training_set = en_training_set + fr_training_set\n",
    "\n",
    "# 1) Initialize tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# 2) Train from the iterator\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(training_set, batch_size=1000),\n",
    "    vocab_size=32_000,  # Choose your vocab size\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]  # Or any set of special tokens you prefer\n",
    ")\n",
    "\n",
    "# 3) Save the tokenizer\n",
    "tokenizer.save(\"my_bytelevel_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 480, 3960, 3114, 1289,  291, 4272]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "['The', 'Ä quick', 'Ä bro', 'wn', 'Ä f', 'ox']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"my_bytelevel_tokenizer.json\",  # or the two files from the BPE approach\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\"\n",
    ")\n",
    "\n",
    "encoded_input = hf_tokenizer(\"The quick brown fox\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "decoded_output = hf_tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "\n",
    "print(encoded_input)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "max_len = 256\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Transformer model...\n",
      "> Number of parameters: 79,658,240\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (76) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 58\u001b[0m\n\u001b[0;32m     51\u001b[0m tgt_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[0;32m     52\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfull_like(tgt_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m1\u001b[39m], hf_tokenizer\u001b[38;5;241m.\u001b[39mbos_token_id), \n\u001b[0;32m     53\u001b[0m     tgt_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     54\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 2) TODO: Forward pass through the Transformer\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 3) TODO: Shift labels to the right (teacher forcing)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m labels \u001b[38;5;241m=\u001b[39m tgt_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, cross_attn_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m src_mask\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, src_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m build_decoder_mask(tgt_mask, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m---> 17\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m logits, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, attn_weights\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# TODO: Implement the forward pass through the encoder layers\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# x shape: [batch_size, seq_len, embed_dim]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# 1) Multi-head self-attention\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_output  \u001b[38;5;66;03m# residual\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 54\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# TODO: apply mask if given\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# TODO: normalize attention_scores\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Softmax along the last dimension (seq_len of K)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (76) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the Transformer model...\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Print the number of parameters\n",
    "print(f\"> Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Compute total number of batches in the training set\n",
    "num_batches = int(np.ceil(len(dataset['train']) / batch_size))\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=hf_tokenizer.pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Switch model to training mode\n",
    "model.train()\n",
    "for epoch_num in range(epochs):\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        train_batch = dataset['train'][batch_idx * batch_size : (batch_idx + 1) * batch_size]['translation']\n",
    "\n",
    "        src_train = [example['fr'] for example in train_batch]\n",
    "        tgt_train = [example['en'] for example in train_batch]\n",
    "        \n",
    "        src_tokens = hf_tokenizer(\n",
    "            src_train, \n",
    "            return_tensors='pt', \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len\n",
    "        ).to(device)\n",
    "\n",
    "        tgt_tokens = hf_tokenizer(\n",
    "            tgt_train, \n",
    "            return_tensors='pt', \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len\n",
    "        ).to(device)\n",
    "\n",
    "        # 1) TODO: Prepend BOS token to target tokens\n",
    "        tgt_tokens[\"input_ids\"] = torch.cat([\n",
    "            torch.full_like(tgt_tokens[\"input_ids\"][:, :1], hf_tokenizer.bos_token_id), \n",
    "            tgt_tokens[\"input_ids\"][:, :-1]\n",
    "        ], dim=1)\n",
    "\n",
    " \n",
    "        # 2) TODO: Forward pass through the Transformer\n",
    "        logits, _ = model(\n",
    "            src=src_tokens['input_ids'],\n",
    "            tgt=tgt_tokens[\"input_ids\"],\n",
    "            src_mask=src_tokens[\"attention_mask\"],\n",
    "            tgt_mask=tgt_tokens[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        # 3) TODO: Shift labels to the right (teacher forcing)\n",
    "        labels = tgt_tokens[\"input_ids\"][:, 1:].contiguous()\n",
    "        logits = logits[:, :-1].contiguous()\n",
    "        \n",
    "        # 4) TODO: Compute loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # 5) TODO: Backpropagate and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "\n",
    "    print(f\"Epoch {epoch_num} Finished! Avg Loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Saving model checkpoint...\")\n",
    "torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.lineplot(x=range(len(train_losses)), y=train_losses, marker='o', markersize=5)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Examples\n",
    "\n",
    "Generate using greedy decoding. You will learn more about it in class later in the semester.\n",
    "\n",
    "Here is a [link for a checkpoint](https://drive.google.com/file/d/130dDwMBJGhvSEFQdHkxaU-R5IjlSbO2j/view?usp=sharing) that you can use for prediction and attention weight visualization.\n",
    "It was trained using the following configuration\n",
    "```\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 2048\n",
    "num_layers = 4\n",
    "vocab_size = 32000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'transformer_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m      3\u001b[0m     src_vocab_size\u001b[38;5;241m=\u001b[39mhf_tokenizer\u001b[38;5;241m.\u001b[39mvocab_size, \n\u001b[0;32m      4\u001b[0m     tgt_vocab_size\u001b[38;5;241m=\u001b[39mhf_tokenizer\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     15\u001b[0m src_val \u001b[38;5;241m=\u001b[39m [dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transformer_model.pt'"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=hf_tokenizer.vocab_size, \n",
    "    tgt_vocab_size=hf_tokenizer.vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "state_dict = torch.load(\"transformer_model.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "src_val = [dataset['train'][i]['translation']['fr'] for i in range(5)]\n",
    "tgt_val = [dataset['train'][i]['translation']['en'] for i in range(5)]\n",
    "\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        # Prepare inputs\n",
    "        src = src_tokens_val['input_ids'][i].unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[hf_tokenizer.bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "        # Greedy decode up to max_len\n",
    "        for step in range(max_len):\n",
    "            # Create tgt padding mask\n",
    "            tgt_mask = torch.ones_like(tgt).bool().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Get the most probable token at the current step\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "            # Append\n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # End conditions\n",
    "            if next_token.item() in [hf_tokenizer.eos_token_id, hf_tokenizer.pad_token_id]:\n",
    "                break\n",
    "\n",
    "        # Decode\n",
    "        translation = hf_tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Source:      {src_val[i]}\")\n",
    "        print(f\"  Translation: {translation}\")\n",
    "        print(f\"  Target:      {tgt_val[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLXazZQx5roi"
   },
   "source": [
    "## Analyze Cross-Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "900FaGrbMCms"
   },
   "source": [
    "- Choose one sample sentence, output attention weights for each token using heatmap\n",
    "- Which pairs of the token have the greatest attention weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:26.595566Z",
     "iopub.status.busy": "2024-02-22T15:18:26.594626Z",
     "iopub.status.idle": "2024-02-22T15:18:27.145253Z",
     "shell.execute_reply": "2024-02-22T15:18:27.144212Z",
     "shell.execute_reply.started": "2024-02-22T15:18:26.595530Z"
    },
    "id": "QQMMFUipeVkd",
    "outputId": "f5822cd3-9fdf-4af5-9333-1eef3fad1c4e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_cross_attention(\n",
    "    attn_weights, \n",
    "    source_tokens, \n",
    "    target_tokens, \n",
    "    batch_idx=0, \n",
    "    head_idx=0, \n",
    "    title=\"Cross-Attention\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize cross-attention weights for a given batch and head.\n",
    "\n",
    "    Args:\n",
    "        attn_weights: Tensor of shape [batch_size, num_heads, tgt_len, src_len]\n",
    "            Cross-attention weights from your Transformer decoder, \n",
    "            typically returned alongside logits in a (logits, attn_weights) tuple.\n",
    "        source_tokens: List of source tokens (strings) for the batch_idx sample.\n",
    "        target_tokens: List of target tokens (strings) for the batch_idx sample.\n",
    "        batch_idx: Which batch element to visualize (default=0).\n",
    "        head_idx: Which attention head to visualize (default=0).\n",
    "        title: Title for the plot.\n",
    "\n",
    "    Example Usage:\n",
    "        # Suppose attn_weights has shape [batch_size, num_heads, tgt_len, src_len]\n",
    "        # and you have the corresponding token lists for the source and target:\n",
    "        visualize_cross_attention(attn_weights, src_tokens, tgt_tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract the attention for the specified batch & head\n",
    "    #    shape: [tgt_len, src_len]\n",
    "    attn = attn_weights[batch_idx, head_idx].detach().cpu().numpy()\n",
    "\n",
    "    tgt_len, src_len = attn.shape\n",
    "\n",
    "    # 2) Plot the heatmap\n",
    "    plt.figure(figsize=(min(12, 1 + 0.5 * src_len), min(6, 1 + 0.5 * tgt_len)))\n",
    "    sns.heatmap(attn, \n",
    "                vmin=0.0, vmax=1.0, \n",
    "                cmap=\"Blues\", \n",
    "                xticklabels=source_tokens, \n",
    "                yticklabels=target_tokens, \n",
    "                cbar=True)\n",
    "\n",
    "    plt.title(f\"{title} (batch={batch_idx}, head={head_idx})\")\n",
    "    plt.xlabel(\"Source Tokens\")\n",
    "    plt.ylabel(\"Target Tokens\")\n",
    "\n",
    "    # Rotate the x-axis labels if tokens are long\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T15:18:40.105997Z",
     "iopub.status.busy": "2024-02-22T15:18:40.105643Z",
     "iopub.status.idle": "2024-02-22T15:18:40.181096Z",
     "shell.execute_reply": "2024-02-22T15:18:40.180179Z",
     "shell.execute_reply.started": "2024-02-22T15:18:40.105972Z"
    },
    "id": "HOxwg2xw52rG",
    "outputId": "f4b013bd-85e3-44ba-eba4-6656db8963cf",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18])\n",
      "torch.Size([1, 15])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiHeadAttention' object has no attribute 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m     logits, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_tokens_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_tokens_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_tokens_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_tokens_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Decode the source and target sequences\u001b[39;00m\n\u001b[0;32m     22\u001b[0m src_tokens \u001b[38;5;241m=\u001b[39m hf_tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(src_tokens_val[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, cross_attn_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m src_mask\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, src_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m build_decoder_mask(tgt_mask, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m---> 17\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m logits, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, enc_output, tgt_mask, cross_attn_mask)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, attn_weights\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# TODO: Implement the forward pass through the encoder layers\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# x shape: [batch_size, seq_len, embed_dim]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# 1) Multi-head self-attention\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_output  \u001b[38;5;66;03m# residual\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 60\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     58\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Apply dropout to attention weights\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(attn_weights)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# TODO: compute attention_output\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# multiply by V \u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# shape: [batch_size, num_heads, seq_len, head_dim]\u001b[39;00m\n\u001b[0;32m     66\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, V)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\mnlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiHeadAttention' object has no attribute 'dropout'"
     ]
    }
   ],
   "source": [
    "src_val = \"Le renard brun rapide saute par-dessus le chien paresseux.\"\n",
    "tgt_val = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the source and target\n",
    "src_tokens_val = hf_tokenizer(src_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "tgt_tokens_val = hf_tokenizer(tgt_val, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "print(src_tokens_val[\"input_ids\"].shape)\n",
    "print(tgt_tokens_val[\"input_ids\"].shape)\n",
    "\n",
    "# Run the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(\n",
    "        src=src_tokens_val['input_ids'],\n",
    "        tgt=tgt_tokens_val['input_ids'],\n",
    "        src_mask=src_tokens_val['attention_mask'],\n",
    "        tgt_mask=tgt_tokens_val['attention_mask']\n",
    "    )\n",
    "\n",
    "# Decode the source and target sequences\n",
    "src_tokens = hf_tokenizer.convert_ids_to_tokens(src_tokens_val[\"input_ids\"][0])\n",
    "tgt_tokens = hf_tokenizer.convert_ids_to_tokens(tgt_tokens_val[\"input_ids\"][0])\n",
    "\n",
    "# Visualize the attention weights\n",
    "for i in range(num_heads):\n",
    "    visualize_cross_attention(\n",
    "        attn_weights=attn_weights,\n",
    "        source_tokens=src_tokens,\n",
    "        target_tokens=tgt_tokens,\n",
    "        batch_idx=0,\n",
    "        head_idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpXzr36fLUtI"
   },
   "source": [
    "## Congrats! You can now train a simple machine translator by your own ;)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4480106,
     "sourceId": 7679312,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
